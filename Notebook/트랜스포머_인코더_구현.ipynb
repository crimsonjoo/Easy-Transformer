{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 트랜스포머 인코더 구현하기\n",
        "[Crimsonjoo 깃허브](https://github.com/crimsonjoo/Easy-Transformer)\n",
        "\n",
        "\n",
        "이 노트북은 PyTorch를 사용하여 트랜스포머 모델의 인코더 부분을 구현하는 방법을 단계별로 설명합니다.\n",
        "\n",
        "트랜스포머 모델은 주로 자연어 처리(NLP) 분야에서 사용되는 모델로, 여러 개의 인코더와 디코더 레이어로 구성됩니다.\n",
        "\n",
        "해당 노트북에서는 인코더 부분만을 구현해보겠습니다.\n"
      ],
      "metadata": {
        "id": "-_MXXY1M_fDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;"
      ],
      "metadata": {
        "id": "LVolkSTjD9rS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 필요한 라이브러리 불러오기\n",
        "\n",
        "먼저, 이 구현에 필요한 PyTorch 라이브러리를 불러옵니다."
      ],
      "metadata": {
        "id": "5tB7MBQd_mAF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ647gJ9-_IS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;"
      ],
      "metadata": {
        "id": "1ZkRbTCgCEVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 인코더 구성 요소 구현하기\n",
        "\n",
        "트랜스포머 인코더를 구현하기 전에, 인코더가 사용하는 핵심 구성 요소들을 먼저 구현합니다.\n",
        "\n",
        "이 구성 요소들은 멀티 헤드 어텐션, 포지션 와이즈 피드포워드 네트워크, 그리고 레이어 노멀라이제이션입니다.\n"
      ],
      "metadata": {
        "id": "IEamfFDbC8My"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**- 스케일드 닷 프로덕트 어텐션 함수**\n",
        "\n",
        "어텐션 메커니즘은 입력 시퀀스의 중요한 부분에 더 많은 주의를 기울이도록 모델을 돕습니다.\n",
        "\n",
        "\n",
        "스케일드 닷 프로덕트 어텐션은 이러한 어텐션 메커니즘의 한 형태입니다.\n"
      ],
      "metadata": {
        "id": "kHd1u0IPCHfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일드 닷 프로덕트 어텐션 함수\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]  # 키 벡터의 차원\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)  # 스케일링\n",
        "    if mask is not None:\n",
        "        scaled += mask  # 마스크가 있으면 추가\n",
        "    attention = F.softmax(scaled, dim=-1)  # 소프트맥스를 통한 어텐션 가중치 계산\n",
        "    values = torch.matmul(attention, v)  # 가중치를 값 벡터에 적용\n",
        "    return values, attention"
      ],
      "metadata": {
        "id": "kcCyEP9VCHkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**- 멀티 헤드 어텐션 클래스**\n",
        "\n",
        "멀티 헤드 어텐션은 모델이 다양한 위치에서 정보를 동시에 얻을 수 있도록 합니다.\n",
        "\n",
        "이는 모델의 표현력을 향상시키는 데 도움이 됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "xpDpLu04DopZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 멀티 헤드 어텐션 클래스\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)  # Q, K, V를 위한 선형 변환\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)  # 최종 선형 변환\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        qkv = self.qkv_layer(x).view(batch_size, seq_len, self.num_heads, 3 * self.head_dim)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        q, k, v = [tensor.permute(0, 2, 1, 3) for tensor in (q, k, v)]  # 헤드 차원을 앞으로 이동\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, -1)\n",
        "        return self.linear_layer(values)"
      ],
      "metadata": {
        "id": "xJBTzn1vDopZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**- 레이어 노멀라이제이션 & 포지션 와이즈 피드포워드 네트워크**\n",
        "\n",
        "레이어 노멀라이제이션은 각 레이어의 입력을 정규화하여 학습을 안정화시키는 기술입니다.\n",
        "\n",
        "포지션 와이즈 피드포워드 네트워크는 각 위치에서 독립적으로 작동하는 두 개의 선형 변환과 ReLU 활성화 함수로 구성됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "A0b8B3DTD_b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이어 노멀라이제이션 클래스\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "\n",
        "# 포지션 와이즈 피드포워드 네트워크 클래스\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear1(x)))\n",
        "        return self.linear2(x)"
      ],
      "metadata": {
        "id": "3y7QMGOqD_b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**- 인코더 레이어 클래스**\n",
        "\n",
        "인코더 레이어는 멀티 헤드 어텐션과 포지션 와이즈 피드포워드 네트워크를 포함합니다.\n",
        "\n",
        "이 레이어는 입력 시퀀스를 변환하여 다음 레이어로 전달하는 역할을 합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "iTK5nWO6D_8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 레이어 클래스\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = LayerNormalization(d_model)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.norm2 = LayerNormalization(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x2 = self.norm1(x + self.self_attn(x, mask))\n",
        "        return self.norm2(x2 + self.ffn(x2))"
      ],
      "metadata": {
        "id": "QTXjz5rvD_8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**- 인코더 클래스**\n",
        "\n",
        "이제 모든 구성 요소를 하나로 합쳐 인코더를 구성합니다.\n",
        "\n",
        "인코더는 여러 개의 인코더 레이어를 포함하며, 각 레이어는 입력 데이터를 처리하여 최종 출력을 생성합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "IDHp9ZWgEATq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 클래스\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.norm = LayerNormalization(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "90QapT7eEATq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;"
      ],
      "metadata": {
        "id": "tsYMAfXYFQ1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 모델 실행\n",
        "\n",
        "마지막으로, 인코더 모델을 초기화하고 입력 데이터에 대해 실행해보겠습니다.\n",
        "\n",
        "이를 통해 모델이 정상적으로 작동하는지 확인할 수 있습니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0ftLhhCpFQ1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 config\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5\n",
        "\n",
        "\n",
        "# 모델 선언\n",
        "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
        "\n",
        "\n",
        "# 모델 실행\n",
        "x = torch.randn( (batch_size, max_sequence_length, d_model) ) # includes positional encoding\n",
        "out = encoder(x)"
      ],
      "metadata": {
        "id": "bnV8AWHUFQ1T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}